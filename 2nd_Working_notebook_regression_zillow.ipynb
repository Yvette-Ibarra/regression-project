{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eef34fd",
   "metadata": {},
   "source": [
    "# Working Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e526d4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import QuantileTransformer, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression, SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,LassoLars, TweedieRegressor\n",
    "\n",
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b53408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e848d21",
   "metadata": {},
   "source": [
    "# Acquire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zillow_data():\n",
    "        ''' Acquire Zillow data using properties_2017 table from Code up Data Base. Columns bedroomcnt, \n",
    "            bathroomcnt, calculatedfinishedsquarefeet, taxvaluedollarcnt, yearbuilt, taxamount, fips \n",
    "        '''\n",
    "   \n",
    "         # sql query for acquisition\n",
    "        sql_query = \"\"\"\n",
    "        SELECT calculatedfinishedsquarefeet,bathroomcnt,bedroomcnt,taxvaluedollarcnt,yearbuilt, fireplacecnt,\n",
    "        decktypeid, poolcnt, garagecarcnt,fips\n",
    "\n",
    "        FROM properties_2017\n",
    "        LEFT JOIN propertylandusetype USING(propertylandusetypeid)\n",
    "        LEFT JOIN predictions_2017 USING(parcelid)\n",
    "        WHERE (propertylandusetype.propertylandusedesc LIKE ('%%Single%%')) \n",
    "            AND (predictions_2017.transactiondate like '2017%%');\n",
    "        \"\"\"\n",
    "        # Acquisition\n",
    "        df = pd.read_sql(sql_query, env.get_connection('zillow'))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to adress % by using %% so it could be read by notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b972b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = get_zillow_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52441 observations\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673422b",
   "metadata": {},
   "source": [
    "# Data Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6f820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a01a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd104997",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "roomcnt                          0\n",
    "    \n",
    "fips                             0\n",
    "    \n",
    "regionidcounty                   0\n",
    "    \n",
    "latitude                         0\n",
    "    \n",
    "propertycountylandusecode        0\n",
    "    \n",
    "longitude                        0\n",
    "    \n",
    "regionidzip                     26\n",
    "    \n",
    "yearbuilt                      116\n",
    "    \n",
    "fullbathcnt                    137\n",
    "    \n",
    "calculatedbathnbr              137\n",
    "\n",
    "I could maybe add one of these to explorations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd92148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4919f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7055f17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> There seems to be bathrooms/ bedrooms with min of 0 and max above what ight be consider a single family dweling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.rename(columns={'bedroomcnt': 'bedrooms','bathroomcnt': 'bathrooms',\n",
    "            'calculatedfinishedsquarefeet': 'squarefeet','taxvaluedollarcnt': 'home_value',\n",
    "                        'lotsizesquarefeet':'lot','fireplacecnt': 'fireplace','decktypeid':'deck','poolcnt':'pool',\n",
    "                       'garagecarcnt':'garage','fips':'county'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fancy_features(df, feature):\n",
    "    df[feature]=df[feature].replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "    if df[feature]:\n",
    "        df[feature] == 1\n",
    "    else:\n",
    "        df[feature].fillna(0)\n",
    "    return df[feature].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34022a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.garage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a2c5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_features = ['fireplace','deck','pool','garage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cbbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fancy_features(df, feature):\n",
    "    df[feature]=df[feature].replace(r\"^\\s*$\", np.nan, regex=True)     \n",
    "    # fill fancy features with 0 assumption that if it was not mark it did not exist\n",
    "    df[feature] = df[feature].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89705633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fancy_features(df):\n",
    "    columnst = ['fireplace','deck','pool','garage']    \n",
    "    for i in columns:\n",
    "        df[feature]=df[feature].replace(r\"^\\s*$\", np.nan, regex=True)     \n",
    "        # fill fancy features with 0 assumption that if it was not mark it did not exist\n",
    "        df[feature] = df[feature].fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in fancy_features:\n",
    "    process_fancy_features(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d1191",
   "metadata": {},
   "source": [
    "These values were filled in with 0 since the presense was not anotated it is assumed it does not exist\n",
    "\n",
    "fireplace     45198\n",
    "\n",
    "deck          52052\n",
    "\n",
    "pool          41345\n",
    "\n",
    "garage        34426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f194e9a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# still have 82 nulls in square feet and 116 nulls in yearbuilt and 1 null in home_value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef134e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12436eab",
   "metadata": {},
   "source": [
    "## outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacecafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df):\n",
    "    \"\"\"Manually handle outliers '\"\"\"\n",
    "    df = df[df.bathrooms <= 6]\n",
    "    \n",
    "    df = df[df.bedrooms <= 6]\n",
    "    \n",
    "    df = df[df.home_value <= 1_750_000]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d611e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5beff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(51918-1632)/(52441)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66df2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1632/ 52441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738c425",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removed homes above 1_750_000 as outliers total of 1632 rows about 3% of data still retain .958 of original dat\n",
    "df[df.home_value > 1_750_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c50bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "df. home_value.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "52112/52441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321547e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "52112-52441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = handle_outliers(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238203ed",
   "metadata": {},
   "source": [
    "# nulls and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.squarefeet.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901fe38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dropped properties with no bathrooms and no bedrooms 153 rows at still retained .990 of original data\n",
    "df= df[~(df.bathrooms==0) & ~(df.bedrooms ==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c778e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "51959 - 52112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54191372",
   "metadata": {},
   "outputs": [],
   "source": [
    "51959/52441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0bc752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['squarefeet'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.squarefeet.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8baa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped null values in yearbuilt 89 and 1 in home_value still retained .99 of original data.\n",
    "# total dropped 90, \n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f563ae6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da826da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "51918/52441"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c14824",
   "metadata": {},
   "source": [
    "### fireplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf2a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fireplace.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac8aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.fireplace = df.fireplace.replace({2:1, 3:1, 4:1, 5:1})\n",
    "df.fireplace.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbb976",
   "metadata": {},
   "source": [
    "### garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb44ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.garage.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34b819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.garage = df.garage.replace({2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 13:1,14:1})\n",
    "df.garage.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b46c3c",
   "metadata": {},
   "source": [
    "### pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b5278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.pool.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e01642",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.deck.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d71b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.deck= df.deck.replace({66:1})\n",
    "df.deck.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccbbda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features (df):\n",
    "    df.fireplace = df.fireplace.replace({2:1, 3:1, 4:1, 5:1})\n",
    "    df.deck= df.deck.replace({66:1})\n",
    "    df.garage = df.garage.replace({2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 13:1,14:1})\n",
    "    df = pd.get_dummies(df, columns=['county','fancy_features'], drop_first=False)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c9d71",
   "metadata": {},
   "source": [
    "# New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df):\n",
    "    #Creating new column for home age using year_built, casting as float\n",
    "    df['home_age'] = 2017- df['yearbuilt']\n",
    "    df[\"home_age\"] = df[\"home_age\"].astype('float')\n",
    "    \n",
    "    df['optional_features'] = (df.garage==1)|(df.deck == 1)|(df.pool == 1)|(df.fireplace == 1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdd3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new column for home age using year_built, casting as float\n",
    "df['home_age'] = 2017- df['yearbuilt']\n",
    "df[\"home_age\"] = df[\"home_age\"].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fancy_features'] = (df.garage==1)|(df.deck == 1)|(df.pool == 1)|(df.fireplace == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc25206e",
   "metadata": {},
   "source": [
    "## FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabeling FIPS data\n",
    "#df['fips'] = df.fips.replace({6037:'Los Angeles',\n",
    " #                  6059:'Orange',\n",
    "  #                 6111:'Ventura'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb2142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b349db",
   "metadata": {},
   "source": [
    "# split data for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    '''\n",
    "    split_data takes in data Frame and splits into  train , validate, test.\n",
    "    The split is 20% test 80% train/validate. Then 30% of 80% validate and 70% of 80% train.\n",
    "    Aproximately (train 56%, validate 24%, test 20%)\n",
    "    Returns train, validate, and test \n",
    "    '''\n",
    "    # split test data from train/validate\n",
    "    train_and_validate, test = train_test_split(df, random_state=123, test_size=.2)\n",
    "\n",
    "    # split train from validate\n",
    "    train, validate = train_test_split(train_and_validate, random_state=123, test_size=.3)\n",
    "                                   \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814df6e",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e21f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534dcd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(train['squarefeet', 'bathrooms', 'bedrooms', 'home_value', 'yearbuilt',\n",
    "  #      'county', 'home_age']\n",
    " #      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correaltion between variables\n",
    "zillow_corr = train.corr(method='spearman')\n",
    "zillow_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf25451",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pass my correlation matrix to Seaborn's heatmap \n",
    "kwargs = {'alpha':.9,\n",
    "          'linewidth':3, \n",
    "          'linestyle':'-',\n",
    "          'linecolor':'black'}\n",
    "sns.heatmap(zillow_corr, cmap='Purples', annot=True, \n",
    "            mask=np.triu(zillow_corr), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf58355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variable_pair(df):\n",
    "    columns = ['squarefeet',\n",
    " 'bathrooms',\n",
    " 'bedrooms',\n",
    " 'home_value',\n",
    " 'yearbuilt',\n",
    " 'fireplace',\n",
    " 'deck',\n",
    " 'pool',\n",
    " 'garage',\n",
    " 'home_age',\n",
    "    'county',\n",
    "              'fancy_features']\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.lmplot(data=df, x=col, y='home_value', line_kws={'color':'red'})\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d78c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_variable_pair(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a3de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_and_continuous_vars(df, cat_vars, cont_vars):\n",
    "    for col in cat_vars:\n",
    "        for col2 in cont_vars:\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,6))\n",
    "            fig.suptitle(f'{col} vs. {col2}')\n",
    "            sns.boxplot(data=df, x=col, y=col2, ax=ax1)\n",
    "            sns.violinplot(data=df, x=col, y=col2, ax=ax2)\n",
    "            sns.barplot(data=df, x=col, y=col2, ax=ax3)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbd8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1280acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set categories\n",
    "cat_vars =['pool','garage','deck','fireplace', 'bathrooms', 'bedrooms','county','fancy_features']\n",
    "cont_vars =['home_value','home_age','squarefeet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c550ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_categorical_and_continuous_vars(train, cat_vars, cont_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0abf5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Home_Value increases with features:\n",
    "    \n",
    "    * pool\n",
    "    \n",
    "    * garage\n",
    "    \n",
    "    * deck\n",
    "    \n",
    "    * fireplace\n",
    "    \n",
    "    * Square feet\n",
    "    \n",
    "    * bathrooms\n",
    "    \n",
    "    * Bedrooms\n",
    "    \n",
    "    * fancy_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e56a65",
   "metadata": {},
   "source": [
    "Questions:\n",
    "       * Answer the following initial question\n",
    "       \n",
    "        * What does the average home look like\n",
    "        \n",
    "        * Do popular builts have a higher home value than the average built.\n",
    "        \n",
    "        * Do properties with more bathrooms have a higher home value? bedrooms? squarefeet?\n",
    "        \n",
    "        * What are the average attributes of home whose value that are in the bottom quantile?\n",
    "        \n",
    "        * Are one stories home more valuable than two story homes.\n",
    "        \n",
    "        * Does having things like pool, deck, fireplace, garage increase home value?\n",
    "        \n",
    "        *( maybe look into roomcnt, regionidcounty, propertycountylandusecode, regionzip,fullbathcnt, yearbuilt, calculatedbathbr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d40d2b",
   "metadata": {},
   "source": [
    "# What does the average home look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cdda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889ba50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in columns:\n",
    "    mean = train[i].mean()\n",
    "    print (f'{i} mean = {mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791bd39",
   "metadata": {},
   "source": [
    "The average home_value is 433,444\n",
    "The average home is 1835 **squarefeet**, has between 2 to 2.5 **bathrooms**, and 3 **bedrooms**.\n",
    "About 1 in 3 homes have a **garage** on average\n",
    "and 1 in 5 homes have a **pool** on average\n",
    "1 in 7 have a **fireplace**\n",
    "and less tha 1% have a **deck**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3974dac3",
   "metadata": {},
   "source": [
    "# What is the most popular built?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a9bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in columns:\n",
    "    mode = train[i].mode()\n",
    "    print (f'{i} mode = {mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8f31f",
   "metadata": {},
   "source": [
    "The most popular built is a home with 1_120 squarefeet, 2 bathrooms, 3 bedrooms,  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692bb41",
   "metadata": {},
   "source": [
    "# Do popular builts have a higher home value than others? More house vs minimal house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1766e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = train[ (train.bathrooms ==2)&(train.bedrooms ==3)]\n",
    "house.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd765759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a50e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "more_house = train[ (train.bathrooms>2)&(train.bedrooms >3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b80999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minimal_house = train[(train.bathrooms<2)&(train.bedrooms <3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90617402",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_house.home_value.median(), minimal_house.home_value.median(), house.home_value.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b8d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_house.home_value.mean(), minimal_house.home_value.mean(), house.home_value.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0445d86",
   "metadata": {},
   "source": [
    "# Homes with the ideal amount of bathrooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a46dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.bathrooms.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "house = train[train.bathrooms == 2]\n",
    "more_house = train[ train.bathrooms>2]\n",
    "\n",
    "minimal_house = train[train.bathrooms<2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_house.home_value.median(), minimal_house.home_value.median(), house.home_value.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58345a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "more_house.home_value.mean(), minimal_house.home_value.mean(), house.home_value.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8f190",
   "metadata": {},
   "source": [
    "#  What are the average attributes of home whose value that are in the bottom quantile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_q = train[train.home_value < train.home_value.quantile(.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_q.bedrooms.mean(), bottom_q.bathrooms.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_q.bathrooms.mean(), bottom_q.bathrooms.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763df437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bottom_q.home_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e52739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cat_vs_cont(df, cat_vars=cat_vars, cont_vars=cont_vars):\n",
    "    print('Categorical vs Continuous Variables:')\n",
    "    #number = 1\n",
    "    palettes = ['flare', 'Blues_r', 'PuRd_r', 'Accent']\n",
    "    for j, cont in enumerate(cont_vars):\n",
    "        plt.figure(figsize=(20,4))\n",
    "        plt.suptitle(cont)\n",
    "        for i, cat in enumerate(cat_vars):\n",
    "            plt.subplot(1, 4, i+1)\n",
    "            sns.barplot(data=df, x=cat, y=cont, palette=palettes[j])\n",
    "            plt.title(cat + ' vs ' + cont)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_cat_vs_cont(train, cat_vars,cont_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1aef4",
   "metadata": {},
   "source": [
    "# Does having an extra feature in a home raise home value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6877105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['fancy_features'] = (train.garage==1)|(train.deck == 1)|(train.pool == 1)|(train.fireplace == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cdefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=train, x='fancy_features', y='home_value');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253342b",
   "metadata": {},
   "source": [
    "# Is fips a driver of home value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0670360",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=train, x='county', y='home_value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f1a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8cccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c42af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67dafff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(train.corr(method='spearman')[['home_value']].sort_values(by='home_value', ascending=False), vmin=-1, vmax=1, annot=True, cmap='Purples')\n",
    "heatmap.set_title('Features Correlating with Home Price', fontdict={'fontsize':18}, pad=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e713c55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">Highest correlation seems to be squarefeet, bathrooms and fancy_features, yearbuilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc74281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = train.columns.to_list()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9ddc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in columns:    \n",
    " #   sns.displot(train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e82c9",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e113779",
   "metadata": {},
   "outputs": [],
   "source": [
    "above_mean_bathrooms = train.bathrooms[train.bathrooms > train.bathrooms.mean()]\n",
    "overall_mean = train.bathrooms.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd57ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha=0.05\n",
    "t, p = stats.ttest_1samp(above_mean_bathrooms, overall_mean)\n",
    "\n",
    "print(t, p/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e838b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearsonr r for continuous variables\n",
    "for i in cont_vars:\n",
    "    α =0.05\n",
    "    corr, p = stats.pearsonr(train[i], train.home_value)\n",
    "    print('_____________________________________________________')\n",
    "    print('HYPOTHESIS')\n",
    "    print(f'H0: There is no significant difference between tax_value in different {i}')\n",
    "    print(f'Ha:(There is a significant difference between tax_value in different {i}')\n",
    "    print(f'{i} correlation {corr}, p-val{p}')\n",
    "    if p < α:\n",
    "        print('We reject the null hypothesis.')\n",
    "        print(f'There is a correlation between home_value and {i}')\n",
    "    else:\n",
    "        print('We fail to reject the null hypothesis.')\n",
    "        print(f'There is no correlation between home_value and {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0278ee09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in cat_vars:\n",
    "    α =0.05\n",
    "    corr, p = stats.pearsonr(train[i], train.home_value)\n",
    "    print('_____________________________________________________')\n",
    "    print('HYPOTHESIS')\n",
    "    print(f'H0: There is no significant difference between tax_value in different {i}')\n",
    "    print(f'Ha:(There is a significant difference between tax_value in different {i}')\n",
    "    print(f'{i} correlation {corr}, p-val{p}')\n",
    "    if p < α:\n",
    "        print('We reject the null hypothesis.')\n",
    "        print(f'There is a correlation between tax_value and {i}')\n",
    "    else:\n",
    "        print('We fail to reject the null hypothesis.')\n",
    "        print(f'There is no correlation between tax_value and {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fbd858",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns.remove('home_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80ba7a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabeling FIPS data\n",
    "train['county'] = train.county.replace({6037:'Los Angeles',\n",
    "                       6059:'Orange',\n",
    "                       6111:'Ventura'})\n",
    "# Creating Dummy Variables from County\n",
    "train = pd.get_dummies(train, columns=['county','fancy_features'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabeling FIPS data\n",
    "validate['county'] = validate.county.replace({6037:'Los Angeles',\n",
    "                       6059:'Orange',\n",
    "                       6111:'Ventura'})\n",
    "# Creating Dummy Variables from County\n",
    "validate = pd.get_dummies(validate, columns=['county','fancy_features'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3792980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabeling FIPS data\n",
    "test['county'] = test.county.replace({6037:'Los Angeles',\n",
    "                       6059:'Orange',\n",
    "                       6111:'Ventura'})\n",
    "# Creating Dummy Variables from County\n",
    "test= pd.get_dummies(test, columns=['county','fancy_features'], drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c821ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f43ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c564dce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e7b43",
   "metadata": {},
   "source": [
    "# VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['squarefeet', 'home_age', 'county_Los Angeles',\n",
    "        'fancy_features_True']]\n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "  \n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                          for i in range(len(X.columns))]\n",
    "  \n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17cb977",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379982a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(train, \n",
    "               validate, \n",
    "               test, \n",
    "               columns_to_scale=['squarefeet','bathrooms','bedrooms','yearbuilt','home_age']):\n",
    "    '''\n",
    "    scale_data takes in train , validate, test data  and returns their scaled counterparts.\n",
    "    '''\n",
    "    # create copies of our original data\n",
    "    train_scaled = train.copy()\n",
    "    validate_scaled = validate.copy()\n",
    "    test_scaled = test.copy()\n",
    "    #create the scaler\n",
    "    scaler = QuantileTransformer(output_distribution='normal')\n",
    "    # fit the scaler into train data\n",
    "    scaler.fit(train[columns_to_scale])\n",
    "    \n",
    "    # applying the scaler to train, validate, and test data\n",
    "    train_scaled[columns_to_scale] = pd.DataFrame(scaler.transform(train[columns_to_scale]),\n",
    "                                                  columns=train[columns_to_scale].columns.values).set_index([train.index.values])\n",
    "                                                  \n",
    "    validate_scaled[columns_to_scale] = pd.DataFrame(scaler.transform(validate[columns_to_scale]),\n",
    "                                                  columns=validate[columns_to_scale].columns.values).set_index([validate.index.values])\n",
    "    \n",
    "    test_scaled[columns_to_scale] = pd.DataFrame(scaler.transform(test[columns_to_scale]),\n",
    "                                                 columns=test[columns_to_scale].columns.values).set_index([test.index.values])\n",
    "    \n",
    "    return train_scaled, validate_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c80f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled, X_validate_scaled, X_test_scaled = scale_data(train,validate,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08fc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7097b",
   "metadata": {},
   "source": [
    "# Set up X and y target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aed2a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(train.home_value)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a414ad",
   "metadata": {},
   "outputs": [],
   "source": [
    " columns_to_scale=['squarefeet','bathrooms','bedrooms','yearbuilt','home_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns_to_scale:\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.hist(data=train, x= i,bins=20)\n",
    "    plt.title(f'Original {i}')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.hist(x= i, data=X_train_scaled,bins=20)\n",
    "    plt.title(f'Quantile Transformation Normal {i}')\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bdb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup X and y\n",
    "X_train_scaled = X_train_scaled.drop(columns='home_value')\n",
    "y_train = train.home_value\n",
    "\n",
    "X_validate_scaled = X_validate_scaled.drop(columns='home_value')\n",
    "y_validate = validate.home_value\n",
    "\n",
    "X_test_scaled = X_test_scaled.drop(columns='home_value')\n",
    "y_test = test.home_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3340b9ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_test_scaled.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c5bee",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225b2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# We need y_train and y_validate to be dataframes to append the new columns with predicted values. \n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)\n",
    "\n",
    "# 1. Predict HV_pred_mean\n",
    "HV_pred_mean = y_train.home_value.mean()\n",
    "y_train['HV_pred_mean'] = HV_pred_mean\n",
    "y_validate['HV_pred_mean'] = HV_pred_mean\n",
    "\n",
    "# 2. compute G3_pred_median\n",
    "HV_pred_median = y_train.home_value.median()\n",
    "y_train['HV_pred_median'] = HV_pred_median\n",
    "y_validate['HV_pred_median'] = HV_pred_median\n",
    "\n",
    "# 3. RMSE of G3_pred_mean\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_mean)**(1/2)\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_mean)**(1/2)\n",
    "\n",
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))\n",
    "\n",
    "# 4. RMSE of G3_pred_median\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_median)**(1/2)\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_median)**(1/2)\n",
    "\n",
    "print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c843db6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87967e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.hist(y_train.home_value, color='blue', alpha=.5, label='Actual home value')\n",
    "plt.hist(y_train.HV_pred_mean, bins=1, color='red', alpha=.5, rwidth=100, label=\"Predicted home value - Mean\")\n",
    "plt.hist(y_train.HV_pred_median, bins=1, color='black', alpha=.5, rwidth=100, label=\"Predicted Final Grades - Median\")\n",
    "plt.xlabel(\"home_value\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02d9d9",
   "metadata": {},
   "source": [
    "# OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255b2d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm.fit(X_train_scaled, y_train.home_value)\n",
    "\n",
    "# predict train\n",
    "y_train['HV_pred_lm'] = lm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_lm)**(1/2)\n",
    "# predict validate\n",
    "y_validate['HV_pred_lm'] = lm.predict(X_validate_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71e883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_lm)**(1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b2ef1",
   "metadata": {},
   "source": [
    "# Lasso Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a39e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=1.0)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lars.fit(X_train_scaled, y_train.home_value)\n",
    "\n",
    "# predict train\n",
    "y_train['HV_pred_lars'] = lars.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_lars)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['HV_pred_lars'] = lars.predict(X_validate_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09b794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_lars)**(1/2)\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9d823",
   "metadata": {},
   "source": [
    "# TweedieRegressor (GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power=1, alpha=0)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "glm.fit(X_train_scaled, y_train.home_value)\n",
    "\n",
    "# predict train\n",
    "y_train['HV_pred_glm'] = glm.predict(X_train_scaled)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_glm)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['HV_pred_glm'] = glm.predict(X_validate_scaled)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_glm)**(1/2)\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=1 & alpha=0\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2ef4b",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree2 = pf.fit_transform(X_train_scaled)\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree2 = pf.transform(X_validate_scaled)\n",
    "X_test_degree2 = pf.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d057b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm2 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm2.fit(X_train_degree2, y_train.home_value)\n",
    "\n",
    "# predict train\n",
    "y_train['HV_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_pred_lm2)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['HV_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_pred_lm2)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e6c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22282fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e65f69c",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca46abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(y_predicted):\n",
    "    return mean_squared_error(y_train.home_value, y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "def calculate_RMSE(y_predicted):\n",
    "    return mean_squared_error(y_train.home_value,y_predicted)**.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfff445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.apply(calculate_mse).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027c17f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_train.apply(calculate_RMSE).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62199b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6407e2",
   "metadata": {},
   "source": [
    "BEST MODELS are  LM2 and then GLM then LM:\n",
    "\n",
    "HV_pred_lm2      264,401.437\n",
    "\n",
    "HV_pred_glm      272,993.029\n",
    "\n",
    "HV_pred_lm       280,034.483\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f81a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def regression_errors(y, yhat):\n",
    "    '''\n",
    "    regression_errors takes in actual value  of target y  and predicted value yhat \n",
    "    and returns  SSE, ESS, TSS, MSE, RMSE\n",
    "    y: actual values of target\n",
    "    yhat: predicted value of target\n",
    "    \n",
    "    Return :\n",
    "        * SSE Sum or Squared error\n",
    "        * ESS Explained sum of squares\n",
    "        * TSS Total sum of squares\n",
    "        * MSE Mean squared error\n",
    "        * RMSE Root mean squared error\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # calculations\n",
    "    MSE = mean_squared_error(y, yhat)\n",
    "    SSE = MSE * len(y)\n",
    "    RMSE = MSE**.5\n",
    "    ESS = ((yhat - y.mean())**2).sum()\n",
    "    TSS = ESS + SSE\n",
    "    \n",
    "    return f'SSE = {SSE}', f'ESS = {ESS}', f'TSS = {TSS}', f'MSE = {MSE}', f'RMSE = {RMSE}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_validate.columns.to_list():\n",
    "    print(i)\n",
    "    print (regression_errors(y_validate.home_value, y_validate[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5241444",
   "metadata": {},
   "source": [
    "## Best models is HV_pred_lm2, then HV_pred_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde23dd2",
   "metadata": {},
   "source": [
    "# ADD MORE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae931ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree3 = pf.fit_transform(X_train_scaled)\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree3 = pf.transform(X_validate_scaled)\n",
    "X_test_degree3 = pf.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc26d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm4 = LinearRegression(normalize=True)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm4.fit(X_train_degree3, y_train.home_value)\n",
    "\n",
    "# predict train\n",
    "y_train['HV_degree3'] = lm4.predict(X_train_degree3)\n",
    "\n",
    "# evaluate: rms\n",
    "rmse_train = mean_squared_error(y_train.home_value, y_train.HV_degree3)**(1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['HV_degree3'] = lm4.predict(X_validate_degree3)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.home_value, y_validate.HV_degree3)**(1/2)\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9272b8",
   "metadata": {},
   "source": [
    "# Multiple Regression + RFE 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = LinearRegression()\n",
    "\n",
    "\n",
    "# 1. Transform our X\n",
    "rfe = RFE(lm3, n_features_to_select=6)\n",
    "rfe.fit(X_train_scaled, y_train.home_value)\n",
    "print('selected top 6 features:', X_train_scaled.columns[rfe.support_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe = rfe.transform(X_train_scaled)\n",
    "# 2. Use the transformed x in our model\n",
    "lm3.fit(X_train_rfe, y_train.home_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23559a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c555322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Make predictions\n",
    "\n",
    "X_validate_rfe = rfe.transform(X_validate_scaled)\n",
    "y_train['multiple_rfe'] = lm3.predict(X_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b887e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validate_rfe = pd.DataFrame(X_validate_rfe, columns = X_validate_scaled.columns[rfe.support_], index = X_validate_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate['multiple_rfe'] = lm3.predict(X_validate_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d623dd94",
   "metadata": {},
   "source": [
    "### Select Features for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_Kbest(X, y, k=2):   \n",
    "    '''\n",
    "    select_Kbest takes in :\n",
    "    X: a dataframe representing numerical independent features\n",
    "    y: a pandas Series representing a target variable\n",
    "    k: a keyword argument defaulted to 2 for the number of features\n",
    "    \n",
    "    returns: a list of the selected features using SelectBest \n",
    "    '''\n",
    "    # model\n",
    "    kbest = SelectKBest(f_regression, k=k)\n",
    "    # fit\n",
    "    kbest.fit(X, y)\n",
    "    # mask of selection\n",
    "    feature_mask = kbest.get_support()\n",
    "    rank = X.columns[feature_mask]\n",
    "    \n",
    "    # convert to list\n",
    "    rank = rank.to_list()\n",
    "\n",
    "\n",
    "    return f'TOP {k} features: {rank}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_Kbest(X_train_scaled,y_train.home_value, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f837506",
   "metadata": {},
   "source": [
    "### Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\n",
    "    'actual': validate.tax_value\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X must be 2-d array\n",
    "#X_train_scaled = X_train_scaled[['bathrooms']]\n",
    "# y can be 1-d array\n",
    "#y_train = train.tax_value\n",
    "\n",
    "# 1. make the thing\n",
    "lm = LinearRegression()\n",
    "# 2. fit the thing\n",
    "lm.fit(X_train_scaled[['bathrooms']], y_train)\n",
    "# 3. use the thing (make predictions)\n",
    "#X_validate = validate[['taxamount']]\n",
    "predictions['simple_lm'] = lm.predict(X_validate[['bathrooms']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into modesl make up \n",
    "lm.coef_, lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d24a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'tax_value = {lm.coef_}*bathrooms + {lm.intercept_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bac608",
   "metadata": {},
   "source": [
    "### Multiple + RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe(X, y, k=2):\n",
    "    ''' \n",
    "    rfe takes in:\n",
    "    X: a dataframe representing numerical independent features\n",
    "    y: a pandas Series representing a target variable\n",
    "    k: a keyword argument defaulted to 2 for the number of features\n",
    "    \n",
    "    returns: a list of the selected features using RFE\n",
    "    '''\n",
    "    # Model\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select= k)\n",
    "    #fit\n",
    "    rfe.fit(X, y)\n",
    "    # mask selection\n",
    "    mask = rfe.get_support()\n",
    "    \n",
    "    return X.columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133e602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfe(X_train_scaled, y_train, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lm = LinearRegression()\n",
    "k = 2\n",
    "\n",
    "### 1. Transform our X\n",
    "rfe = RFE(lm, n_features_to_select=2)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "print('selected top 2 features:', X_train_scaled.columns[rfe.support_])\n",
    "X_train_rfe = rfe.transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ec918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transform our X\n",
    "X_train_rfe = rfe.transform(X_train_scaled)\n",
    "# 2. Use the transformed x in our model\n",
    "lm.fit(X_train_rfe, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "X_validate_rfe = rfe.transform(X_validate)\n",
    "predictions['multiple_rfe'] = lm.predict(X_validate_rfe)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into models make up\n",
    "lm.coef_, lm.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f239b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'tax_value = {lm.coef_[0]:.2f} x squarefeet + {lm.coef_[1]:.2f} x bedroom + {lm.intercept_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f7902",
   "metadata": {},
   "source": [
    "### Poly Degree2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Polynomial Features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "poly.fit(X_train_scaled)\n",
    "X_train_poly = pd.DataFrame(\n",
    "    poly.transform(X_train_scaled),\n",
    "    columns=poly.get_feature_names(X_train_scaled.columns),\n",
    "    index=train.index,\n",
    ")\n",
    "X_train_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6fba86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Use the features\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_poly, y_train)\n",
    "\n",
    "X_validate_poly = poly.transform(X_validate_scaled)\n",
    "predictions['polynomial degree 2'] = lm.predict(X_validate_poly)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342486a",
   "metadata": {},
   "source": [
    "# Poly interactions_only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1256f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "poly.fit(X_train_scaled)\n",
    "X_train_poly = pd.DataFrame(\n",
    "    poly.transform(X_train),\n",
    "    columns=poly.get_feature_names(X_train.columns),\n",
    "    index=train.index,\n",
    ")\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train_poly, y_train)\n",
    "\n",
    "X_validate_poly = poly.transform(X_validate_scaled)\n",
    "predictions['polynomial only interaction'] = lm.predict(X_validate_poly)\n",
    "\n",
    "pd.Series(lm.coef_, index=poly.get_feature_names(X_train_scaled.columns)).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76797922",
   "metadata": {},
   "source": [
    "## Laso Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=1)\n",
    "\n",
    "# fit the model to our training data\n",
    "lars.fit(X_train, y_train)\n",
    "\n",
    "# predict validate\n",
    "X_validate_pred_lars = lars.predict(X_validate)\n",
    "\n",
    "# Add lassolars predictions to our predictions DataFrame\n",
    "predictions['lasso_lars'] = X_validate_pred_lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850e55f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8be426",
   "metadata": {},
   "source": [
    "# GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06088dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power=1, alpha=0)\n",
    "\n",
    "# fit the model to our training data\n",
    "glm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict validate\n",
    "X_validate_predict_glm = glm.predict(X_validate_scaled)\n",
    "\n",
    "# Add lassolars predictions to our predictions DataFrame\n",
    "predictions['glm'] = X_validate_predict_glm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3456260",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a baseline model\n",
    "predictions['baseline'] = train.tax_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c1eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36e633",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(y_predicted):\n",
    "    return mean_squared_error(predictions.actual, y_predicted)\n",
    "\n",
    "predictions.apply(calculate_mse).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89bed2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "def calculate_RMSE(y_predicted):\n",
    "    return mean_squared_error(predictions.acual,y_predicted)\n",
    "predictions.apply(calculate_mse).sort_values()**.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4bbd4d",
   "metadata": {},
   "source": [
    "First itteration of models Polynomial degree2 was best, Then glm, then lasso_lars then baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
